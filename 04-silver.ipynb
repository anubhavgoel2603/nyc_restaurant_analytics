{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1eff9e5a-26a0-47a6-8538-8a470539c954",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ./01-config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "58540ab1-b487-425b-a7c2-e2953a389fb8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ./02-setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "03258459-f3ce-4969-9cad-bb65cb433f6d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from datetime import datetime\n",
    "from pyspark.sql.functions import lit, current_timestamp, col, regexp_replace, upper, sha2, concat_ws\n",
    "import requests\n",
    "import uuid\n",
    "\n",
    "class Silver():\n",
    "    def __init__(self):\n",
    "\n",
    "        print(\"Loading configuration settings...\")\n",
    "        conf = Config()\n",
    "        self.nyc_population_by_community_bronze_table = \"nyc_population_by_community_raw\"\n",
    "        self.nyc_restaurant_inspection_bronze_table = \"nyc_restaurant_inspection_raw\"\n",
    "        self.silver_path = conf.storage_account + \"silver_db/\"\n",
    "        self.catalog_name = conf.catalog_name\n",
    "        self.bronze_db_name = \"bronze_db\"\n",
    "        self.db_name = \"silver_db\"\n",
    "        print(\"âœ… Configuration Loaded:\")\n",
    "        print(f\"   - Storage Path for Silver Layer: {self.silver_path}\")\n",
    "        print(f\"   - Catalog Name: {self.catalog_name}\")\n",
    "        print(f\"   - Database Name: {self.db_name}\")\n",
    "        print(\"ðŸš€ Silver Ingestion Initialized Successfully! ðŸŽ¯\\n\")\n",
    "\n",
    "    def table_exists(self, silver_table_name):\n",
    "        \"\"\"Check if a Delta table exists in the catalog.\"\"\"\n",
    "        try:\n",
    "            spark.table(silver_table_name)\n",
    "            return True\n",
    "        except:\n",
    "            return False\n",
    "    \n",
    "    def dim_borough_population(self):\n",
    "        \"\"\"Load NYC Borough Population data into the Silver layer with incremental loading.\"\"\"\n",
    "        \n",
    "        print(\"ðŸ”¹ Starting silver layer data ingestion for NYC Borough Population...\")\n",
    "\n",
    "        bronze_table_name = f\"{self.catalog_name}.{self.bronze_db_name}.nyc_population_by_community_raw\"\n",
    "        silver_table_name = f\"{self.catalog_name}.{self.db_name}.dim_borough_population\"\n",
    "        dim_borough_population_storage_path = self.silver_path + \"dim_borough_population/\"\n",
    "        ingestion_timestamp = datetime.utcnow()\n",
    "\n",
    "        if self.table_exists(silver_table_name):\n",
    "            print(f\"ðŸ“Œ Table {silver_table_name} exists. Performing incremental load...\")\n",
    "            existing_df = spark.table(silver_table_name)\n",
    "            print(f\"ðŸ” Existing record count: {existing_df.count()}\")\n",
    "\n",
    "            df = spark.read.table(bronze_table_name)\n",
    "            columns = df.columns\n",
    "            population_cols = [col_name for col_name in df.columns if \"population\" in col_name]\n",
    "            df_unpivoted = df.melt([\"borough\", \"cd_name\"], population_cols, \"year\", \"population\")\n",
    "            final_df = df_unpivoted.withColumn(\"borough\", upper(col(\"borough\")))\\\n",
    "                                  .withColumn(\"cd_name\", upper(col(\"cd_name\")))\\\n",
    "                                  .withColumn(\"year\", regexp_replace(col(\"year\"), \"^_|_population\", \"\"))\\\n",
    "                                  .withColumn(\"year\", col(\"year\").cast(\"int\"))\\\n",
    "                                  .withColumn(\"population\", col(\"population\").cast(\"int\"))\\\n",
    "                                  .withColumn(\"source\", lit(bronze_table_name))\\\n",
    "                                  .withColumn(\"ingestion_timestamp\", lit(ingestion_timestamp))\n",
    "\n",
    "            new_df = final_df.join(existing_df,[\"borough\", \"cd_name\", \"year\"], \"left_anti\")\n",
    "            \n",
    "            new_records_count = max(new_df.count() - existing_df.count(), 0)\n",
    "            print(f\"ðŸ†• New records to insert: {new_records_count}\")\n",
    "\n",
    "            if new_records_count > 0:\n",
    "                df.write.format(\"delta\").mode(\"append\").saveAsTable(silver_table_name)\n",
    "                print(f\"âœ… {new_records_count} new records inserted into {silver_table_name}.\")\n",
    "            else:\n",
    "                print(\"âœ… No new records to insert. Data is already up-to-date.\")\n",
    "\n",
    "        else:\n",
    "            print(f\"ðŸ› ï¸ Table {silver_table_name} does not exist. Creating Table...\")\n",
    "            spark.sql(f\"\"\"\n",
    "                           CREATE TABLE IF NOT EXISTS {silver_table_name}(\n",
    "                               borough STRING,\n",
    "                               cd_name STRING,\n",
    "                               year INT,\n",
    "                               population INT,\n",
    "                               source STRING,\n",
    "                               ingestion_timestamp TIMESTAMP)\n",
    "                            USING DELTA\n",
    "                            LOCATION '{dim_borough_population_storage_path}'\n",
    "                        \"\"\")\n",
    "            print(f\"âœ… Table {silver_table_name} created successfully!\")\n",
    "            print(\"Performing first-time load...\")\n",
    "\n",
    "            df = spark.read.table(bronze_table_name)\n",
    "\n",
    "            columns = df.columns\n",
    "\n",
    "            population_cols = [col_name for col_name in df.columns if \"population\" in col_name]\n",
    "\n",
    "            df_unpivoted = df.melt([\"borough\", \"cd_name\"], population_cols, \"year\", \"population\")\n",
    "\n",
    "            final_df = df_unpivoted.withColumn(\"borough\", upper(col(\"borough\")))\\\n",
    "                                  .withColumn(\"cd_name\", upper(col(\"cd_name\")))\\\n",
    "                                  .withColumn(\"year\", regexp_replace(col(\"year\"), \"^_|_population\", \"\"))\\\n",
    "                                  .withColumn(\"year\", col(\"year\").cast(\"int\"))\\\n",
    "                                  .withColumn(\"population\", col(\"population\").cast(\"int\"))\\\n",
    "                                  .withColumn(\"source\", lit(bronze_table_name))\\\n",
    "                                  .withColumn(\"ingestion_timestamp\", lit(ingestion_timestamp))\n",
    "\n",
    "            final_df.write.mode(\"overwrite\").saveAsTable(silver_table_name)\n",
    "            print(f\"âœ… First-time load completed! {final_df.count()} rows loaded successfully from {bronze_table_name} to {silver_table_name}\")\n",
    "\n",
    "\n",
    "    def dim_restaurant(self):\n",
    "        \"\"\"Load NYC Restaurant data into the Silver layer with incremental loading.\"\"\"\n",
    "        \n",
    "        print(\"ðŸ”¹ Starting silver layer data ingestion for NYC Restaurant data...\")\n",
    "\n",
    "        bronze_table_name = f\"{self.catalog_name}.{self.bronze_db_name}.nyc_restaurant_inspection_raw\"\n",
    "        silver_table_name = f\"{self.catalog_name}.{self.db_name}.dim_restaurant\"\n",
    "        dim_restaurant_storage_path = self.silver_path + \"dim_restaurant/\"\n",
    "        ingestion_timestamp = datetime.utcnow()\n",
    "\n",
    "        bronze_data = spark.sql(f\"\"\"\n",
    "                                    SELECT DISTINCT\n",
    "                                        CAST(CAMIS AS INT) AS restaurant_id,\n",
    "                                        UPPER(DBA) AS restaurant_name,\n",
    "                                        NVL(UPPER(CUISINE_DESCRIPTION), 'NOT AVAILABLE') AS cuisine_type,\n",
    "                                        NVL(PHONE, 'NOT AVAILABLE') AS phone_no,\n",
    "                                        BUILDING AS building_no, \n",
    "                                        STREET AS street_name,\n",
    "                                        CASE WHEN UPPER(BORO) = '0' THEN 'NOT AVAILABLE' ELSE UPPER(BORO) END AS borough,\n",
    "                                        ZIPCODE AS zipcode,\n",
    "                                        CAST(longitude AS DOUBLE) AS longitude,\n",
    "                                        CAST(latitude AS DOUBLE) AS latitude\n",
    "                                    FROM {bronze_table_name}\n",
    "                                \"\"\")\n",
    "\n",
    "        if self.table_exists(silver_table_name):\n",
    "            print(f\"ðŸ“Œ Table {silver_table_name} exists. Performing incremental load...\")\n",
    "\n",
    "            df = bronze_data.withColumn(\"source\", lit(bronze_table_name))\\\n",
    "                        .withColumn(\"ingestion_timestamp\", lit(ingestion_timestamp))\n",
    "\n",
    "            df.createOrReplaceTempView(\"temp_view\")\n",
    "\n",
    "            existing_df = spark.table(silver_table_name)\n",
    "            existing_df = existing_df.drop(\"source\", \"ingestion_timestamp\", \"start_date\", \"end_date\")\n",
    "            print(f\"ðŸ” Existing record count: {existing_df.count()}\")\n",
    "            new_df = bronze_data.exceptAll(existing_df)\n",
    "            new_records_count = new_df.count()\n",
    "            print(f\"ðŸ†• New records to insert: {new_records_count}\")\n",
    "\n",
    "            if new_records_count > 0:\n",
    "                spark.sql(f\"\"\"\n",
    "                                MERGE INTO {silver_table_name} AS TARGET\n",
    "                                USING temp_view AS SOURCE\n",
    "                                ON TARGET.restaurant_id = SOURCE.restaurant_id AND TARGET.END_DATE IS NULL\n",
    "                                WHEN MATCHED AND TARGET.INGESTION_TIMESTAMP < SOURCE.INGESTION_TIMESTAMP THEN \n",
    "                                UPDATE SET TARGET.END_DATE = CURRENT_TIMESTAMP()\n",
    "                                WHEN NOT MATCHED THEN \n",
    "                                INSERT (\n",
    "                                    restaurant_id, restaurant_name, cuisine_type, phone_no, building_no, \n",
    "                                    street_name, borough, zipcode, longitude, latitude, source, ingestion_timestamp, start_date, end_date\n",
    "                                ) \n",
    "                                VALUES (\n",
    "                                    SOURCE.restaurant_id, SOURCE.restaurant_name, SOURCE.cuisine_type, SOURCE.phone_no, \n",
    "                                    SOURCE.building_no, SOURCE.street_name, SOURCE.borough, SOURCE.zipcode, \n",
    "                                    SOURCE.longitude, SOURCE.latitude, '{bronze_table_name}', \n",
    "                                    {ingestion_timestamp}, current_timestamp(), null\n",
    "                                )\n",
    "                            \"\"\")\n",
    "                print(f\"âœ… {new_records_count} new records inserted into {silver_table_name}.\")\n",
    "            else:\n",
    "                print(\"âœ… No new records to insert. Data is already up-to-date.\")\n",
    "\n",
    "        else:\n",
    "            print(f\"ðŸ› ï¸ Table {silver_table_name} does not exist. Creating Table...\")\n",
    "            spark.sql(f\"\"\"\n",
    "                           CREATE TABLE IF NOT EXISTS {silver_table_name} (\n",
    "                                restaurant_id INT,\n",
    "                                restaurant_name STRING,\n",
    "                                cuisine_type STRING,\n",
    "                                phone_no STRING,\n",
    "                                building_no STRING,\n",
    "                                street_name STRING,\n",
    "                                borough STRING,\n",
    "                                zipcode STRING,\n",
    "                                longitude DOUBLE,\n",
    "                                latitude DOUBLE,\n",
    "                                source STRING,\n",
    "                                ingestion_timestamp TIMESTAMP,\n",
    "                                start_date TIMESTAMP,\n",
    "                                end_date TIMESTAMP)\n",
    "                            USING DELTA\n",
    "                            LOCATION '{dim_restaurant_storage_path}'\n",
    "                        \"\"\")\n",
    "            print(f\"âœ… Table {silver_table_name} created successfully!\")\n",
    "            print(\"Performing first-time load...\")\n",
    "\n",
    "            df = bronze_data.withColumn(\"source\", lit(bronze_table_name))\\\n",
    "                            .withColumn(\"ingestion_timestamp\", lit(ingestion_timestamp))\\\n",
    "                            .withColumn(\"start_date\", current_timestamp())\\\n",
    "                            .withColumn(\"end_date\", lit(None).cast(\"timestamp\"))\n",
    "\n",
    "            df.write.mode(\"overwrite\").saveAsTable(silver_table_name)\n",
    "            print(f\"âœ… First-time load completed! {df.count()} rows loaded successfully from {bronze_table_name} to {silver_table_name}\")\n",
    "    \n",
    "    def dim_voilation(self):\n",
    "\n",
    "        \"\"\"Load NYC Voilation data into the Silver layer with incremental loading.\"\"\"\n",
    "        \n",
    "        print(\"ðŸ”¹ Starting silver layer data ingestion for NYC Voilation codes data...\")\n",
    "\n",
    "        bronze_table_name = f\"{self.catalog_name}.{self.bronze_db_name}.nyc_restaurant_inspection_raw\"\n",
    "        silver_table_name = f\"{self.catalog_name}.{self.db_name}.dim_voilation\"\n",
    "        dim_voilation_storage_path = self.silver_path + \"dim_voilation/\"\n",
    "        ingestion_timestamp = datetime.utcnow()\n",
    "\n",
    "        bronze_data = spark.sql(f\"\"\"\n",
    "                                    SELECT VIOLATION_CODE, \n",
    "                                    MAX(UPPER(VIOLATION_DESCRIPTION)) AS VIOLATION_DESCRIPTION\n",
    "                                    FROM {bronze_table_name}\n",
    "                                    WHERE VIOLATION_CODE IS NOT NULL\n",
    "                                    GROUP BY VIOLATION_CODE\n",
    "                                \"\"\")\n",
    "\n",
    "        if self.table_exists(silver_table_name):\n",
    "            print(f\"ðŸ“Œ Table {silver_table_name} exists. Performing incremental load...\")\n",
    "\n",
    "            df = bronze_data.withColumn(\"source\", lit(bronze_table_name))\\\n",
    "                        .withColumn(\"ingestion_timestamp\", lit(ingestion_timestamp))\n",
    "\n",
    "            df.createOrReplaceTempView(\"temp_view\")\n",
    "\n",
    "            existing_df = spark.table(silver_table_name)\n",
    "            existing_df = existing_df.drop(\"source\", \"ingestion_timestamp\", \"start_date\", \"end_date\")\n",
    "            print(f\"ðŸ” Existing record count: {existing_df.count()}\")\n",
    "            new_df = bronze_data.exceptAll(existing_df)\n",
    "            new_records_count = new_df.count()\n",
    "            print(f\"ðŸ†• New records to insert: {new_records_count}\")\n",
    "\n",
    "            if new_records_count > 0:\n",
    "                spark.sql(f\"\"\"\n",
    "                                MERGE INTO {silver_table_name} AS TARGET\n",
    "                                USING temp_view AS SOURCE\n",
    "                                ON TARGET.voilation_code = SOURCE.voilation_code AND TARGET.END_DATE IS NULL\n",
    "                                WHEN MATCHED AND TARGET.INGESTION_TIMESTAMP < SOURCE.INGESTION_TIMESTAMP THEN \n",
    "                                UPDATE SET TARGET.END_DATE = CURRENT_TIMESTAMP()\n",
    "                                WHEN NOT MATCHED THEN \n",
    "                                INSERT (\n",
    "                                    voilation_code, voilation_description, source, ingestion_timestamp, start_date, end_date\n",
    "                                ) \n",
    "                                VALUES (\n",
    "                                    SOURCE.voilation_code, SOURCE.voilation_description, '{bronze_table_name}', \n",
    "                                    {ingestion_timestamp}, current_timestamp(), null\n",
    "                                )\n",
    "                            \"\"\")\n",
    "                print(f\"âœ… {new_records_count} new records inserted into {silver_table_name}.\")\n",
    "            else:\n",
    "                print(\"âœ… No new records to insert. Data is already up-to-date.\")\n",
    "\n",
    "        else:\n",
    "            print(f\"ðŸ› ï¸ Table {silver_table_name} does not exist. Creating Table...\")\n",
    "            spark.sql(f\"\"\"\n",
    "                           CREATE TABLE IF NOT EXISTS {silver_table_name} (\n",
    "                                violation_code STRING,\n",
    "                                violation_description STRING,\n",
    "                                source STRING,\n",
    "                                ingestion_timestamp TIMESTAMP,\n",
    "                                start_date TIMESTAMP,\n",
    "                                end_date TIMESTAMP)\n",
    "                            USING DELTA\n",
    "                            LOCATION '{dim_voilation_storage_path}'\n",
    "                        \"\"\")\n",
    "            print(f\"âœ… Table {silver_table_name} created successfully!\")\n",
    "            print(\"Performing first-time load...\")\n",
    "\n",
    "            df = bronze_data.withColumn(\"source\", lit(bronze_table_name))\\\n",
    "                            .withColumn(\"ingestion_timestamp\", lit(ingestion_timestamp))\\\n",
    "                            .withColumn(\"start_date\", current_timestamp())\\\n",
    "                            .withColumn(\"end_date\", lit(None).cast(\"timestamp\"))\n",
    "\n",
    "            df.write.mode(\"overwrite\").saveAsTable(silver_table_name)\n",
    "            print(f\"âœ… First-time load completed! {df.count()} rows loaded successfully from {bronze_table_name} to {silver_table_name}\")\n",
    "    \n",
    "    def fact_restaurant_inspections(self):\n",
    "\n",
    "        \"\"\"Load NYC Restaurant Inspection data into the Silver layer with incremental loading.\"\"\"\n",
    "        \n",
    "        print(\"ðŸ”¹ Starting silver layer data ingestion for NYC Restaurant Inspection data...\")\n",
    "\n",
    "        bronze_table_name = f\"{self.catalog_name}.{self.bronze_db_name}.nyc_restaurant_inspection_raw\"\n",
    "        silver_table_name = f\"{self.catalog_name}.{self.db_name}.fact_restaurant_inspections\"\n",
    "        fact_restaurant_inspections_storage_path = self.silver_path + \"fact_restaurant_inspections/\"\n",
    "        ingestion_timestamp = datetime.utcnow()\n",
    "\n",
    "        bronze_data = spark.sql(f\"\"\"\n",
    "                                    SELECT \n",
    "                                    camis as restaurant_id,\n",
    "                                    coalesce(violation_code, 'NO VIOLATIONS') as violation_code, \n",
    "                                    cast(inspection_date as DATE) as inspection_date,\n",
    "                                    coalesce(upper(inspection_type), 'NONE') as inspection_type,\n",
    "                                    upper(critical_flag) as critical_flag,\n",
    "                                    coalesce(upper(action), 'NONE') as action, \n",
    "                                    cast(score as INT) as score, \n",
    "                                    coalesce(grade, 'NOT AVAILABLE') as grade,\n",
    "                                    cast(grade_date as DATE) as grade_date,\n",
    "                                    cast(record_date as DATE) as record_date\n",
    "                                    FROM {bronze_table_name}\n",
    "                                \"\"\")\n",
    "\n",
    "        if self.table_exists(silver_table_name):\n",
    "            print(f\"ðŸ“Œ Table {silver_table_name} exists. Performing incremental load...\")\n",
    "\n",
    "            key_columns = [col for col in bronze_data.columns if col != \"ingestion_timestamp\"]\n",
    "\n",
    "            df = (bronze_data\n",
    "                    .withColumn(\"unique_hash\", sha2(concat_ws(\"|\", *[bronze_data[col] for col in key_columns]), 256))\n",
    "                    .withColumn(\"source\", lit(bronze_table_name))\n",
    "                    .withColumn(\"ingestion_timestamp\", lit(ingestion_timestamp))\n",
    "                    .dropDuplicates([\"unique_hash\"])\n",
    "                )\n",
    "\n",
    "            df.createOrReplaceTempView(\"temp_view\")\n",
    "\n",
    "            existing_df = spark.table(silver_table_name)\n",
    "            existing_df = existing_df.drop(\"source\", \"ingestion_timestamp\")\n",
    "            bronze_df = df.drop(\"source\", \"ingestion_timestamp\")\n",
    "            print(f\"ðŸ” Existing record count: {existing_df.count()}\")\n",
    "            new_df = bronze_df.exceptAll(existing_df)\n",
    "            new_records_count = new_df.count()\n",
    "            print(f\"ðŸ†• New records to insert: {new_records_count}\")\n",
    "\n",
    "            if new_records_count > 0:\n",
    "                spark.sql(f\"\"\"\n",
    "                                MERGE INTO {silver_table_name} AS TARGET\n",
    "                                USING temp_view AS SOURCE\n",
    "                                ON TARGET.unique_hash = SOURCE.unique_hash\n",
    "                                WHEN MATCHED AND TARGET.INGESTION_TIMESTAMP < SOURCE.INGESTION_TIMESTAMP THEN \n",
    "                                    UPDATE SET *\n",
    "                                WHEN NOT MATCHED THEN \n",
    "                                INSERT *\n",
    "                            \"\"\")\n",
    "                print(f\"âœ… {new_records_count} new records inserted into {silver_table_name}.\")\n",
    "            else:\n",
    "                print(\"âœ… No new records to insert. Data is already up-to-date.\")\n",
    "\n",
    "        else:\n",
    "            print(f\"ðŸ› ï¸ Table {silver_table_name} does not exist. Creating Table...\")\n",
    "            spark.sql(f\"\"\"\n",
    "                           CREATE TABLE IF NOT EXISTS {silver_table_name} (\n",
    "                                restaurant_id STRING,\n",
    "                                violation_code STRING,\n",
    "                                inspection_date DATE,\n",
    "                                inspection_type STRING,\n",
    "                                critical_flag STRING,\n",
    "                                action STRING,\n",
    "                                score INT,\n",
    "                                grade STRING,\n",
    "                                grade_date DATE,\n",
    "                                record_date DATE,\n",
    "                                unique_hash STRING,\n",
    "                                source STRING,\n",
    "                                ingestion_timestamp TIMESTAMP)\n",
    "                            USING DELTA\n",
    "                            LOCATION '{fact_restaurant_inspections_storage_path}'\n",
    "                        \"\"\")\n",
    "            print(f\"âœ… Table {silver_table_name} created successfully!\")\n",
    "            print(\"Performing first-time load...\")\n",
    "\n",
    "            key_columns = [col for col in bronze_data.columns if col != \"ingestion_timestamp\"]\n",
    "\n",
    "            # Generate Unique Hash Column using SHA-256\n",
    "            df = (bronze_data\n",
    "                    .withColumn(\"unique_hash\", sha2(concat_ws(\"|\", *[bronze_data[col] for col in key_columns]), 256))\n",
    "                    .withColumn(\"source\", lit(bronze_table_name))\n",
    "                    .withColumn(\"ingestion_timestamp\", lit(ingestion_timestamp))\n",
    "                    .dropDuplicates([\"unique_hash\"])\n",
    "                )\n",
    "\n",
    "            df.write.mode(\"overwrite\").saveAsTable(silver_table_name)\n",
    "            print(f\"âœ… First-time load completed! {df.count()} rows loaded successfully from {bronze_table_name} to {silver_table_name}\")\n",
    "\n",
    "    def create_silver_tables(self):\n",
    "\n",
    "        setup = Setup(\"silver_db\")\n",
    "        setup.create_db()\n",
    "\n",
    "        self.dim_restaurant()\n",
    "        self.dim_borough_population()\n",
    "        self.dim_voilation()\n",
    "        self.fact_restaurant_inspections()\n",
    "\n",
    "        print(f\"ðŸŽ¯ Data processing for Silver completed successfully.\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6758110355396348,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "04-silver",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
