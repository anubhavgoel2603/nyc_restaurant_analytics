{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1eff9e5a-26a0-47a6-8538-8a470539c954",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ./01-config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "03258459-f3ce-4969-9cad-bb65cb433f6d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import lit, current_timestamp, col, regexp_replace\n",
    "import requests\n",
    "import uuid\n",
    "\n",
    "class Silver():\n",
    "    def __init__(self):\n",
    "        print(\"Initializing Spark Session...\")\n",
    "        self.spark = SparkSession.builder.appName(\"SilverIngestion\").getOrCreate()\n",
    "        app_name = self.spark.conf.get(\"spark.app.name\", \"SilverIngestion\")\n",
    "        print(f\"ðŸ”¹ Spark App Name: {app_name}\")\n",
    "\n",
    "        print(\"Loading configuration settings...\")\n",
    "        conf = Config()\n",
    "        self.nyc_population_by_community_bronze_table = \"nyc_population_by_community_raw\"\n",
    "        self.nyc_restaurant_inspection_bronze_table = \"nyc_restaurant_inspection_raw\"\n",
    "        self.silver_path = conf.storage_account + \"silver_db/\"\n",
    "        self.catalog_name = conf.catalog_name\n",
    "        self.bronze_db_name = \"bronze_db\"\n",
    "        self.db_name = \"silver_db\"\n",
    "        print(\"âœ… Configuration Loaded:\")\n",
    "        print(f\"   - Storage Path for Silver Layer: {self.silver_path}\")\n",
    "        print(f\"   - Catalog Name: {self.catalog_name}\")\n",
    "        print(f\"   - Database Name: {self.db_name}\")\n",
    "        print(\"ðŸš€ Silver Ingestion Initialized Successfully! ðŸŽ¯\\n\")\n",
    "\n",
    "    def table_exists(self, silver_table_name):\n",
    "        \"\"\"Check if a Delta table exists in the catalog.\"\"\"\n",
    "        try:\n",
    "            self.spark.table(silver_table_name)\n",
    "            return True\n",
    "        except:\n",
    "            return False\n",
    "    \n",
    "    def dim_borough_population(self):\n",
    "        \"\"\"Load NYC Borough Population data into the Silver layer with incremental loading.\"\"\"\n",
    "        process_id = str(uuid.uuid4())\n",
    "        print(\"ðŸ”¹ Starting silver layer data ingestion for NYC Borough Population...\")\n",
    "\n",
    "        bronze_table_name = f\"{self.catalog_name}.{self.bronze_db_name}.nyc_population_by_community_raw\"\n",
    "        silver_table_name = f\"{self.catalog_name}.{self.db_name}.dim_borough_population\"\n",
    "        dim_borough_population_storage_path = self.silver_path + \"dim_borough_population/\"\n",
    "\n",
    "        if self.table_exists(silver_table_name):\n",
    "            print(f\"ðŸ“Œ Table {silver_table_name} exists. Performing incremental load...\")\n",
    "            existing_df = self.spark.table(silver_table_name)\n",
    "            print(f\"ðŸ” Existing record count: {existing_df.count()}\")\n",
    "\n",
    "            df = spark.read.table(bronze_table_name)\n",
    "            columns = df.columns\n",
    "            population_cols = [col_name for col_name in df.columns if \"population\" in col_name]\n",
    "            df_unpivoted = df.melt([\"borough\", \"cd_name\"], population_cols, \"year\", \"population\")\n",
    "            final_df = df_unpivoted.withColumn(\"borough\", col(\"borough\").upper())\\\n",
    "                                  .withColumn(\"cd_name\", col(\"cd_name\").upper())\\\n",
    "                                  .withColumn(\"year\", regexp_replace(col(\"year\"), \"^_|_population\", \"\"))\\\n",
    "                                  .withColumn(\"year\", col(\"year\").cast(\"int\"))\\\n",
    "                                  .withColumn(\"population\", col(\"population\").cast(\"int\"))\\\n",
    "                                  .withColumn(\"process_id\", lit(process_id))\\\n",
    "                                  .withColumn(\"source\", lit(bronze_table_name))\\\n",
    "                                  .withColumn(\"ingestion_timestamp\", current_timestamp())\n",
    "            new_df = final_df.join(existing_df,[\"borough\", \"cd_name\", \"year\"], \"left_anti\")\n",
    "            \n",
    "            new_records_count = max(new_df.count() - existing_df.count(), 0)\n",
    "            print(f\"ðŸ†• New records to insert: {new_records_count}\")\n",
    "\n",
    "            if new_records_count > 0:\n",
    "                df.write.format(\"delta\").mode(\"append\").saveAsTable(silver_table_name)\n",
    "                print(f\"âœ… {new_records_count} new records inserted into {silver_table_name}.\")\n",
    "            else:\n",
    "                print(\"âœ… No new records to insert. Data is already up-to-date.\")\n",
    "\n",
    "        else:\n",
    "            print(f\"ðŸ› ï¸ Table {silver_table_name} does not exist. Creating Table...\")\n",
    "            self.spark.sql(f\"\"\"\n",
    "                           CREATE TABLE IF NOT EXISTS {silver_table_name}(\n",
    "                               borough STRING,\n",
    "                               cd_name STRING,\n",
    "                               year INT,\n",
    "                               population INT,\n",
    "                               process_id STRING,\n",
    "                               source STRING,\n",
    "                               ingestion_timestamp TIMESTAMP)\n",
    "                            USING DELTA\n",
    "                            LOCATION '{dim_borough_population_storage_path}'\n",
    "                        \"\"\")\n",
    "            print(f\"âœ… Table {silver_table_name} created successfully!\")\n",
    "            print(\"Performing first-time load...\")\n",
    "\n",
    "            df = spark.read.table(bronze_table_name)\n",
    "\n",
    "            columns = df.columns\n",
    "\n",
    "            population_cols = [col_name for col_name in df.columns if \"population\" in col_name]\n",
    "\n",
    "            df_unpivoted = df.melt([\"borough\", \"cd_name\"], population_cols, \"year\", \"population\")\n",
    "\n",
    "            final_df = df_unpivoted.withColumn(\"borough\", col(\"borough\").upper())\\\n",
    "                                  .withColumn(\"cd_name\", col(\"cd_name\").upper())\\\n",
    "                                  .withColumn(\"year\", regexp_replace(col(\"year\"), \"^_|_population\", \"\"))\\\n",
    "                                  .withColumn(\"year\", col(\"year\").cast(\"int\"))\\\n",
    "                                  .withColumn(\"population\", col(\"population\").cast(\"int\"))\\\n",
    "                                  .withColumn(\"process_id\", lit(process_id))\\\n",
    "                                  .withColumn(\"source\", lit(bronze_table_name))\\\n",
    "                                  .withColumn(\"ingestion_timestamp\", current_timestamp())\n",
    "\n",
    "            final_df.write.mode(\"overwrite\").saveAsTable(silver_table_name)\n",
    "            print(f\"âœ… First-time load completed! {final_df.count()} rows loaded successfully from {bronze_table_name} to {silver_table_name}\")\n",
    "\n",
    "\n",
    "    def dim_restaurant(self):\n",
    "        \"\"\"Load NYC Restaurant data into the Silver layer with incremental loading.\"\"\"\n",
    "        process_id = str(uuid.uuid4())\n",
    "        print(\"ðŸ”¹ Starting silver layer data ingestion for NYC Restaurant data...\")\n",
    "\n",
    "        bronze_table_name = f\"{self.catalog_name}.{self.bronze_db_name}.nyc_restaurant_inspection_raw\"\n",
    "        silver_table_name = f\"{self.catalog_name}.{self.db_name}.dim_restaurant\"\n",
    "        dim_restaurant_storage_path = self.silver_path + \"dim_restaurant/\"\n",
    "        base_query = self.spark.sql(f\"\"\"\n",
    "                                    SELECT DISTINCT\n",
    "                                        CAST(CAMIS AS INT) AS restaurant_id,\n",
    "                                        UPPER(DBA) AS restaurant_name,\n",
    "                                        NVL(UPPER(CUISINE_DESCRIPTION), 'NOT AVAILABLE') AS cuisine_type,\n",
    "                                        NVL(PHONE, 'NOT AVAILABLE') AS phone_no,\n",
    "                                        BUILDING AS building_no, \n",
    "                                        STREET AS street_name,\n",
    "                                        CASE WHEN UPPER(BORO) = '0' THEN 'NOT AVAILABLE' ELSE UPPER(BORO) END AS borough,\n",
    "                                        ZIPCODE AS zipcode,\n",
    "                                        CAST(longitude AS DOUBLE) AS longitude,\n",
    "                                        CAST(latitude AS DOUBLE) AS latitude\n",
    "                                    FROM {bronze_table_name}\n",
    "                                \"\"\")\n",
    "\n",
    "        if self.table_exists(silver_table_name):\n",
    "            print(f\"ðŸ“Œ Table {silver_table_name} exists. Performing incremental load...\")\n",
    "\n",
    "            df = base_query.withColumn(\"process_id\", lit(process_id))\\\n",
    "                        .withColumn(\"source\", lit(bronze_table_name))\\\n",
    "                        .withColumn(\"ingestion_timestamp\", current_timestamp())\n",
    "\n",
    "            df.createOrReplaceTempView(\"temp_view\")\n",
    "\n",
    "            existing_df = self.spark.table(silver_table_name)\n",
    "            existing_df = existing_df.drop(\"process_id\", \"source\", \"ingestion_timestamp\", \"start_date\", \"end_date\")\n",
    "            print(f\"ðŸ” Existing record count: {existing_df.count()}\")\n",
    "            new_df = base_query.exceptAll(existing_df)\n",
    "            new_records_count = new_df.count()\n",
    "            print(f\"ðŸ†• New records to insert: {new_records_count}\")\n",
    "\n",
    "            if new_records_count > 0:\n",
    "                self.spark.sql(f\"\"\"\n",
    "                                MERGE INTO {silver_table_name} AS TARGET\n",
    "                                USING temp_view AS SOURCE\n",
    "                                ON TARGET.restaurant_id = SOURCE.restaurant_id AND TARGET.END_DATE IS NULL\n",
    "                                WHEN MATCHED AND TARGET.INGESTION_TIMESTAMP < SOURCE.INGESTION_TIMESTAMP THEN \n",
    "                                UPDATE SET TARGET.END_DATE = CURRENT_TIMESTAMP()\n",
    "                                WHEN NOT MATCHED THEN \n",
    "                                INSERT (\n",
    "                                    restaurant_id, restaurant_name, cuisine_type, phone_no, building_no, \n",
    "                                    street_name, borough, zipcode, longitude, latitude, process_id, \n",
    "                                    source, ingestion_timestamp, start_date, end_date\n",
    "                                ) \n",
    "                                VALUES (\n",
    "                                    SOURCE.restaurant_id, SOURCE.restaurant_name, SOURCE.cuisine_type, SOURCE.phone_no, \n",
    "                                    SOURCE.building_no, SOURCE.street_name, SOURCE.borough, SOURCE.zipcode, \n",
    "                                    SOURCE.longitude, SOURCE.latitude, '{process_id}', '{bronze_table_name}', \n",
    "                                    current_timestamp(), current_timestamp(), null\n",
    "                                )\n",
    "                            \"\"\")\n",
    "                print(f\"âœ… {new_records_count} new records inserted into {silver_table_name}.\")\n",
    "            else:\n",
    "                print(\"âœ… No new records to insert. Data is already up-to-date.\")\n",
    "\n",
    "        else:\n",
    "            print(f\"ðŸ› ï¸ Table {silver_table_name} does not exist. Creating Table...\")\n",
    "            self.spark.sql(f\"\"\"\n",
    "                           CREATE TABLE IF NOT EXISTS {silver_table_name} (\n",
    "                                restaurant_id INT,\n",
    "                                restaurant_name STRING,\n",
    "                                cuisine_type STRING,\n",
    "                                phone_no STRING,\n",
    "                                building_no STRING,\n",
    "                                street_name STRING,\n",
    "                                borough STRING,\n",
    "                                zipcode STRING,\n",
    "                                longitude DOUBLE,\n",
    "                                latitude DOUBLE,\n",
    "                                process_id STRING,\n",
    "                                source STRING,\n",
    "                                ingestion_timestamp TIMESTAMP,\n",
    "                                start_date TIMESTAMP,\n",
    "                                end_date TIMESTAMP)\n",
    "                            USING DELTA\n",
    "                            LOCATION '{dim_restaurant_storage_path}'\n",
    "                        \"\"\")\n",
    "            print(f\"âœ… Table {silver_table_name} created successfully!\")\n",
    "            print(\"Performing first-time load...\")\n",
    "\n",
    "            df = base_query.withColumn(\"process_id\", lit(process_id))\\\n",
    "                            .withColumn(\"source\", lit(bronze_table_name))\\\n",
    "                            .withColumn(\"ingestion_timestamp\", current_timestamp())\\\n",
    "                            .withColumn(\"start_date\", current_timestamp())\\\n",
    "                            .withColumn(\"end_date\", lit(None).cast(\"timestamp\"))\n",
    "\n",
    "            df.write.mode(\"overwrite\").saveAsTable(silver_table_name)\n",
    "            print(f\"âœ… First-time load completed! {df.count()} rows loaded successfully from {bronze_table_name} to {silver_table_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "99a34d55-d414-495e-a4bf-ddb5c87c1003",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "silver = Silver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "49e95a97-3eef-4bac-ae02-98ff258670b9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "silver.dim_restaurant()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2a7ae11f-9a44-4995-b56d-a3a11e6888a4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE TABLE IF NOT EXISTS anubhav_data_hackathon.silver_db.dim_restaurant (\n",
    "    restaurant_id INT,\n",
    "    restaurant_name STRING,\n",
    "    restaurant_location STRING,\n",
    "    restaurant_building_no STRING,\n",
    "    restaurant_street_name STRING,\n",
    "    restaurant_zipcode STRING,\n",
    "    restaurant_phone STRING,\n",
    "    cuisine_type STRING\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c771f4c0-1007-4e79-8158-65596663fb9a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE TABLE IF NOT EXISTS anubhav_data_hackathon.silver_db.dim_voilation (\n",
    "    violation_code STRING,\n",
    "    critical_flag STRING,\n",
    "    violation_description STRING\n",
    ");\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8c14cb3a-4a8b-43d8-8239-864092ab6729",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "INSERT OVERWRITE anubhav_data_hackathon.silver_db.dim_voilation (\n",
    "    violation_code,\n",
    "    critical_flag,\n",
    "    violation_description\n",
    ")\n",
    "SELECT DISTINCT VIOLATION_CODE, CRITICAL_FLAG, MAX(VIOLATION_DESCRIPTION) AS VIOLATION_DESCRIPTION\n",
    "FROM anubhav_data_hackathon.bronze_db.nyc_restaurant_inspection_raw\n",
    "WHERE VIOLATION_CODE IS NOT NULL\n",
    "GROUP BY VIOLATION_CODE, CRITICAL_FLAG;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e76375bf-8161-4246-82af-26b221724ecd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE TABLE IF NOT EXISTS anubhav_data_hackathon.silver_db.fact_restaurant_inspections (\n",
    "    restaurant_id STRING,\n",
    "    violation_code STRING,\n",
    "    inspection_date DATE,\n",
    "    inspection_type STRING,\n",
    "    action STRING,\n",
    "    score STRING,\n",
    "    grade STRING,\n",
    "    grade_date DATE,\n",
    "    record_date TIMESTAMP\n",
    ") USING DELTA;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "023f37cf-4592-4b65-9e65-2bdd70403506",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "INSERT OVERWRITE anubhav_data_hackathon.silver_db.fact_restaurant_inspections\n",
    "SELECT \n",
    "    a12.restaurant_id,\n",
    "    COALESCE(a13.violation_code, 'No Violations') AS violation_code, \n",
    "    a11.inspection_date,\n",
    "    COALESCE(a11.inspection_type, 'None') AS inspection_type,\n",
    "    COALESCE(a11.action, 'None') AS action, \n",
    "    COALESCE(a11.SCORE, 'Not Available') AS SCORE, \n",
    "    COALESCE(a11.GRADE, 'Not Available') AS GRADE,\n",
    "    CAST(NULLIF(a11.GRADE_DATE, 'Not Available') AS DATE) AS GRADE_DATE,\n",
    "    a11.RECORD_DATE\n",
    "FROM anubhav_data_hackathon.bronze_db.nyc_restaurant_inspection_raw a11\n",
    "JOIN anubhav_data_hackathon.silver_db.dim_restaurant a12\n",
    "  ON a11.camis = a12.restaurant_id\n",
    "LEFT JOIN anubhav_data_hackathon.silver_db.dim_voilation a13\n",
    "  ON a11.violation_code = a13.violation_code;"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 1783798193184668,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "04-silver",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
