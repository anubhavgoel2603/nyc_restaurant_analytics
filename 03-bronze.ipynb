{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fbc4f42a-d8db-4ec1-b865-1d2d645adf56",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ./01-config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1aed92a9-9833-4c9e-b05b-e7c8d1857f6e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import lit, current_timestamp\n",
    "import requests\n",
    "import uuid\n",
    "\n",
    "class Bronze:\n",
    "    def __init__(self):\n",
    "        print(\"Initializing Spark Session...\")\n",
    "        self.spark = SparkSession.builder.appName(\"BronzeIngestion\").getOrCreate()\n",
    "        app_name = self.spark.conf.get(\"spark.app.name\", \"BronzeIngestion\")\n",
    "        print(f\"üîπ Spark App Name: {app_name}\")\n",
    "\n",
    "        print(\"Loading configuration settings...\")\n",
    "        conf = Config()\n",
    "        self.nyc_population_by_community_url = conf.nyc_population_by_community_url\n",
    "        self.nyc_restaurant_inspection_url = conf.nyc_restaurant_inspection_url\n",
    "        self.nyc_restaurant_inspection_bronze_path = conf.storage_account + \"bronze_db/nyc_restaurant_inspection_raw\"\n",
    "        self.nyc_population_bronze_path = conf.storage_account + \"bronze_db/nyc_population_by_community_raw\"\n",
    "        self.catalog_name = conf.catalog_name\n",
    "        self.db_name = \"bronze_db\"\n",
    "        print(\"‚úÖ Configuration Loaded:\")\n",
    "        print(f\"   - NYC Restaurant Inspection URL: {self.nyc_restaurant_inspection_url}\")\n",
    "        print(f\"   - NYC Population by Community URL: {self.nyc_population_by_community_url}\")\n",
    "        print(f\"   - Storage Path for Restaurant Inspection: {self.nyc_restaurant_inspection_bronze_path}\")\n",
    "        print(f\"   - Storage Path for Population Data: {self.nyc_population_bronze_path}\")\n",
    "        print(f\"   - Catalog Name: {self.catalog_name}\")\n",
    "        print(f\"   - Database Name: {self.db_name}\")\n",
    "        print(\"üöÄ Bronze Ingestion Initialized Successfully! üéØ\\n\")\n",
    "\n",
    "    def table_exists(self, table_name):\n",
    "        \"\"\"Check if a Delta table exists in the catalog.\"\"\"\n",
    "        try:\n",
    "            self.spark.table(table_name)\n",
    "            return True\n",
    "        except:\n",
    "            return False\n",
    "\n",
    "    def load_nyc_restaurant_inspections(self):\n",
    "        \"\"\"Load NYC restaurant inspection data into the Bronze layer with incremental loading.\"\"\"\n",
    "        process_id = str(uuid.uuid4())\n",
    "        print(\"üîπ Starting data ingestion for NYC restaurant inspections...\")\n",
    "\n",
    "        print(\"üì° Fetching data from API...\")\n",
    "        count_responses = requests.get(f\"{self.nyc_restaurant_inspection_url}?$select=count(*)\")\n",
    "        if count_responses.status_code == 200:\n",
    "            total_rows = int(count_responses.json()[0][\"count\"])\n",
    "        else:\n",
    "            raise Exception(\"Error fetching Row Count\")\n",
    "\n",
    "        response = requests.get(f\"{self.nyc_restaurant_inspection_url}?$limit={total_rows}\")\n",
    "        if response.status_code != 200:\n",
    "            raise Exception(f\"‚ùå API Error: {response.status_code}\")\n",
    "\n",
    "        data = response.json()\n",
    "        \n",
    "        if not data:\n",
    "            raise Exception(\"‚ö†Ô∏è No Data Returned from API\")\n",
    "\n",
    "        print(f\"‚úÖ Successfully fetched {len(data)} records from API.\")\n",
    "\n",
    "        df = self.spark.createDataFrame(data)\n",
    "        df = df.withColumn(\"process_id\", lit(process_id))\\\n",
    "               .withColumn(\"source_file\", lit(self.nyc_restaurant_inspection_url))\\\n",
    "               .withColumn(\"ingestion_timestamp\", current_timestamp())\n",
    "\n",
    "        bronze_table = f\"{self.catalog_name}.{self.db_name}.nyc_restaurant_inspection_raw\"\n",
    "\n",
    "        if self.table_exists(bronze_table):\n",
    "            print(f\"üìå Table {bronze_table} exists. Performing incremental load...\")\n",
    "            existing_df = self.spark.table(bronze_table)\n",
    "\n",
    "            print(f\"üîç Existing record count: {existing_df.count()}\")\n",
    "            \n",
    "            df = df.join(existing_df, \"inspection_date\", \"left_anti\")\n",
    "\n",
    "            new_records_count = max(df.count() - existing_df.count(), 0)\n",
    "            print(f\"üÜï New records to insert: {new_records_count}\")\n",
    "\n",
    "            if new_records_count > 0:\n",
    "                df.write.format(\"delta\").mode(\"append\").saveAsTable(bronze_table)\n",
    "                print(f\"‚úÖ {new_records_count} new records inserted into {bronze_table}.\")\n",
    "            else:\n",
    "                print(\"‚úÖ No new records to insert. Data is already up-to-date.\")\n",
    "\n",
    "        else:\n",
    "            print(f\"üõ†Ô∏è Table {bronze_table} does not exist. Performing first-time load...\")\n",
    "            df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(bronze_table)\n",
    "            print(f\"‚úÖ Table {bronze_table} created and full dataset loaded.\")\n",
    "\n",
    "        print(f\"üéØ Data processing for {bronze_table} completed successfully.\\n\")\n",
    "\n",
    "    def load_nyc_population_by_community(self):\n",
    "        \"\"\"Load NYC population data into the Bronze layer with incremental loading.\"\"\"\n",
    "        process_id = str(uuid.uuid4())\n",
    "        print(\"üîπ Starting data ingestion for NYC population by community...\")\n",
    "\n",
    "        print(\"üì° Fetching data from API...\")\n",
    "        response = requests.get(self.nyc_population_by_community_url)\n",
    "\n",
    "        if response.status_code != 200:\n",
    "            print(f\"‚ùå API Error: {response.status_code}\")\n",
    "            raise Exception(f\"API Error: {response.status_code}\")\n",
    "\n",
    "        data = response.json()\n",
    "        if not data:\n",
    "            print(\"‚ö†Ô∏è No Data Returned from API\")\n",
    "            raise Exception(\"No Data Returned from API\")\n",
    "\n",
    "        print(f\"‚úÖ Successfully fetched {len(data)} records from API.\")\n",
    "\n",
    "        df = self.spark.createDataFrame(data)\n",
    "        df = df.withColumn(\"process_id\", lit(process_id))\\\n",
    "               .withColumn(\"source_file\", lit(self.nyc_population_by_community_url))\\\n",
    "               .withColumn(\"ingestion_timestamp\", current_timestamp())\n",
    "\n",
    "        bronze_table = f\"{self.catalog_name}.{self.db_name}.nyc_population_by_community_raw\"\n",
    "\n",
    "        if self.table_exists(bronze_table):\n",
    "            print(f\"üìå Table {bronze_table} exists. Performing incremental load...\")\n",
    "            existing_df = self.spark.table(bronze_table)\n",
    "\n",
    "            print(f\"üîç Existing record count: {existing_df.count()}\")\n",
    "\n",
    "            df = df.join(existing_df, [\"cd_name\", \"borough\"], \"left_anti\")\n",
    "\n",
    "            new_records_count = df.count()\n",
    "            print(f\"üÜï New records to insert: {new_records_count}\")\n",
    "\n",
    "            if new_records_count > 0:\n",
    "                df.write.format(\"delta\") \\\n",
    "                        .mode(\"append\") \\\n",
    "                        .option(\"mergeSchema\", \"true\") \\\n",
    "                        .saveAsTable(bronze_table)\n",
    "                print(f\"‚úÖ {new_records_count} new records inserted into {bronze_table}.\")\n",
    "            else:\n",
    "                print(\"‚úÖ No new records to insert. Data is already up-to-date.\")\n",
    "\n",
    "        else:\n",
    "            print(f\"üõ†Ô∏è Table {bronze_table} does not exist. Performing first-time load...\")\n",
    "            df.write.format(\"delta\") \\\n",
    "                    .mode(\"overwrite\") \\\n",
    "                    .option(\"mergeSchema\", \"true\") \\\n",
    "                    .saveAsTable(bronze_table)\n",
    "            print(f\"‚úÖ Table {bronze_table} created and full dataset loaded.\")\n",
    "\n",
    "        print(f\"üéØ Data processing for {bronze_table} completed successfully.\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 7334453071933764,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "03-bronze",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
