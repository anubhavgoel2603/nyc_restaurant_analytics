{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fbc4f42a-d8db-4ec1-b865-1d2d645adf56",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ./01-config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "26df682a-9a4e-4cc5-bc0a-74761dd8311b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ./02-setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1aed92a9-9833-4c9e-b05b-e7c8d1857f6e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from delta.tables import DeltaTable\n",
    "from datetime import datetime\n",
    "from pyspark.sql.functions import lit, current_timestamp, input_file_name, sha2, concat_ws\n",
    "import requests\n",
    "\n",
    "class Bronze:\n",
    "    def __init__(self):\n",
    "        \n",
    "        print(\"Loading Bronze Layer configuration settings...\")\n",
    "        conf = Config()\n",
    "        self.nyc_population_by_community_url = conf.nyc_population_by_community_url\n",
    "        self.nyc_restaurant_inspection_url = conf.nyc_restaurant_inspection_url\n",
    "        self.nyc_restaurant_inspection_bronze_path = conf.storage_account + \"bronze_db/nyc_restaurant_inspection_raw/\"\n",
    "        self.nyc_population_bronze_path = conf.storage_account + \"bronze_db/nyc_population_by_community_raw/\"\n",
    "        self.catalog_name = conf.catalog_name\n",
    "        self.db_name = \"bronze_db\"\n",
    "        print(\"‚úÖ Configuration Loaded:\")\n",
    "        print(f\"   - NYC Restaurant Inspection URL: {self.nyc_restaurant_inspection_url}\")\n",
    "        print(f\"   - NYC Population by Community URL: {self.nyc_population_by_community_url}\")\n",
    "        print(f\"   - Storage Path for Restaurant Inspection: {self.nyc_restaurant_inspection_bronze_path}\")\n",
    "        print(f\"   - Storage Path for Population Data: {self.nyc_population_bronze_path}\")\n",
    "        print(f\"   - Catalog Name: {self.catalog_name}\")\n",
    "        print(f\"   - Database Name: {self.db_name}\")\n",
    "        print(\"üöÄ Bronze Ingestion Initialized Successfully! üéØ\\n\")\n",
    "    \n",
    "    def fetch_and_store_api_data(self, api_url, folder_path):\n",
    "\n",
    "        \"\"\"Load API data into the Bronze layer into json format.\"\"\"\n",
    "\n",
    "        self.api_url = api_url\n",
    "        self.folder_path = folder_path\n",
    "\n",
    "        print(f\"üì° Fetching data from API {self.api_url}...\")\n",
    "        count_responses = requests.get(f\"{self.api_url}?$select=count(*)\")\n",
    "        if count_responses.status_code == 200:\n",
    "            total_rows = int(count_responses.json()[0][\"count\"])\n",
    "        else:\n",
    "            raise Exception(\"Error fetching Row Count\")\n",
    "\n",
    "        response = requests.get(f\"{self.api_url}?$limit={total_rows}\")\n",
    "        if response.status_code != 200:\n",
    "            raise Exception(f\"‚ùå API Error: {response.status_code}\")\n",
    "\n",
    "        data = response.json()\n",
    "        \n",
    "        if not data:\n",
    "            raise Exception(f\"‚ö†Ô∏è No Data Returned from API {self.api_url}.\")\n",
    "\n",
    "        print(f\"‚úÖ Successfully fetched {len(data)} records from API {self.api_url}.\")\n",
    "\n",
    "        df = spark.createDataFrame(data)\n",
    "        df = df.withColumn(\"source_file\", lit(self.api_url))\n",
    "        \n",
    "        # Save as JSON to storage for streaming\n",
    "        df.write.mode(\"overwrite\").json(self.folder_path + \"data_file/\")\n",
    "\n",
    "        print(f\"üéØ Saved the json file at location {self.folder_path}data_file/\")\n",
    "    \n",
    "    def run_api_method(self):\n",
    "        \n",
    "        print(\"üîπ Starting data ingestion for NYC Restaurant Inspection...\")\n",
    "        self.fetch_and_store_api_data(self.nyc_restaurant_inspection_url, self.nyc_restaurant_inspection_bronze_path)\n",
    "\n",
    "        print(\"üîπ Starting data ingestion for NYC Population by Community...\")\n",
    "        self.fetch_and_store_api_data(self.nyc_population_by_community_url, self.nyc_population_bronze_path)\n",
    "    \n",
    "    def enable_schema_evolution(self):\n",
    "        \"\"\"Enable schema evolution for Delta Lake merge operations.\"\"\"\n",
    "\n",
    "        spark.conf.set(\"spark.databricks.delta.schema.autoMerge.enabled\", \"true\")\n",
    "        print(f\"‚úÖ Schema evolution enabled\")\n",
    "    \n",
    "    def batch_writer(self, folder_path, table_name):\n",
    "\n",
    "        self.folder_path = folder_path\n",
    "        self.table_name = table_name\n",
    "        ingestion_timestamp = datetime.utcnow()\n",
    "\n",
    "        df_batch = (spark.read\n",
    "                    .format(\"json\")\n",
    "                    .option(\"inferSchema\", \"true\")\n",
    "                    .load(folder_path + \"data_file/\")\n",
    "                    .withColumn(\"load_date_time\", lit(ingestion_timestamp))\n",
    "                )\n",
    "        \n",
    "        key_columns = [col for col in df_batch.columns if col != \"load_date_time\"]\n",
    "\n",
    "        # Generate Unique Hash Column using SHA-256\n",
    "        df_batch = (df_batch\n",
    "                    .withColumn(\"unique_hash\", sha2(concat_ws(\"|\", *[df_batch[col] for col in key_columns]), 256))\n",
    "                    .dropDuplicates([\"unique_hash\"])\n",
    "                )\n",
    "\n",
    "        \n",
    "        # Define Delta table path\n",
    "        bronze_table_path = f\"{self.catalog_name}.{self.db_name}.{self.table_name}\"\n",
    "\n",
    "        try:\n",
    "            # Load existing Delta table\n",
    "            bronze_table = DeltaTable.forName(spark, bronze_table_path)\n",
    "            print(f\"‚úÖ Found existing table: {bronze_table_path}\")\n",
    "            \n",
    "            # Perform MERGE operation\n",
    "            print(f\"üîÑ Merging new data into {bronze_table_path}...\")\n",
    "            self.enable_schema_evolution()\n",
    "            \n",
    "            (bronze_table.alias(\"bronze\")\n",
    "                .merge(df_batch.alias(\"new_data\"), \"bronze.unique_hash = new_data.unique_hash\")\n",
    "                .whenMatchedUpdateAll()\n",
    "                .whenNotMatchedInsertAll()\n",
    "                .execute()\n",
    "            )\n",
    "\n",
    "            print(f\"‚úÖ Merge operation completed successfully for {bronze_table_path}.\")\n",
    "\n",
    "        except:\n",
    "            print(f\"‚ö†Ô∏è Table {bronze_table_path} does not exist. Creating a new Delta table.\")\n",
    "            # Create new Delta table\n",
    "            (df_batch.write\n",
    "                .format(\"delta\")\n",
    "                .option(\"mergeSchema\", \"true\")\n",
    "                .mode(\"overwrite\")\n",
    "                .saveAsTable(bronze_table_path)\n",
    "            )\n",
    "            print(f\"‚úÖ Created new table: {bronze_table_path}\")\n",
    "\n",
    "     \n",
    "    def load_stream_into_bronze(self):\n",
    "\n",
    "        print(\"üîπ Loading data into Bronze Layer table for NYC Restaurant Inspection...\")\n",
    "        self.batch_writer(folder_path=self.nyc_restaurant_inspection_bronze_path, table_name=\"nyc_restaurant_inspection_raw\")\n",
    "        print(f\"‚úÖ Data successfully loaded into {self.catalog_name}.{self.db_name}.nyc_restaurant_inspection_raw\")\n",
    "\n",
    "        print(\"üîπ Loading data into Bronze Layer table for NYC Borough Population...\")\n",
    "        self.batch_writer(folder_path=self.nyc_population_bronze_path, table_name=\"nyc_population_by_community_raw\")\n",
    "        print(f\"‚úÖ Data successfully loaded into {self.catalog_name}.{self.db_name}.nyc_population_by_community_raw\")\n",
    "\n",
    "    def bronze_layer_execution(self):\n",
    "\n",
    "        \"\"\"Load data into the Bronze layer with incremental loading.\"\"\"\n",
    "        \n",
    "        setup = Setup(\"bronze_db\")\n",
    "        setup.create_db()\n",
    "\n",
    "        self.run_api_method()\n",
    "        self.load_stream_into_bronze()\n",
    "\n",
    "        print(f\"üéØ Data processing for Bronze completed successfully.\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 7334453071933764,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "03-bronze",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
