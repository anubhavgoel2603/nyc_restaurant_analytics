{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ad072975-46a2-4bcb-b8cf-8882a6e21def",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ./01-config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "512b151f-6f22-49e2-a38f-e8854f1c6d68",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ./02-setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "708d17b4-3a54-4272-b1a4-6a4204a749f8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from datetime import datetime\n",
    "from pyspark.sql.functions import lit, current_timestamp, col, regexp_replace, upper, sha2, concat_ws\n",
    "import requests\n",
    "\n",
    "class Gold():\n",
    "    def __init__(self):\n",
    "\n",
    "        print(\"Loading configuration settings...\")\n",
    "        conf = Config()\n",
    "        self.dim_borough_population = \"dim_borough_population\"\n",
    "        self.dim_restaurant = \"dim_restaurant\"\n",
    "        self.dim_violation = \"dim_violation\"\n",
    "        self.fact_restaurant_inspection = \"fact_restaurant_inspections\"\n",
    "        self.gold_path = conf.storage_account + \"gold_db/\"\n",
    "        self.catalog_name = conf.catalog_name\n",
    "        self.silver_db_name = \"silver_db\"\n",
    "        self.db_name = \"gold_db\"\n",
    "        print(\"âœ… Configuration Loaded:\")\n",
    "        print(f\"   - Storage Path for Silver Layer: {self.gold_path}\")\n",
    "        print(f\"   - Catalog Name: {self.catalog_name}\")\n",
    "        print(f\"   - Database Name: {self.db_name}\")\n",
    "        print(\"ðŸš€ Gold Ingestion Initialized Successfully! ðŸŽ¯\\n\")\n",
    "\n",
    "    def table_exists(self, gold_table_name):\n",
    "        \"\"\"Check if a Delta table exists in the catalog.\"\"\"\n",
    "        try:\n",
    "            spark.table(gold_table_name)\n",
    "            return True\n",
    "        except:\n",
    "            return False\n",
    "    \n",
    "    def inspection_detail(self):\n",
    "\n",
    "        \"\"\"Load NYC Detalied inspection data into the Gold layer with incremental loading.\"\"\"\n",
    "        \n",
    "        print(\"ðŸ”¹ Starting gold layer data ingestion for NYC Detalied inspection data...\")\n",
    "\n",
    "        gold_table_name = f\"{self.catalog_name}.{self.db_name}.inspection_detail\"\n",
    "        inspection_detail_storage_path = self.gold_path + \"inspection_detail/\"\n",
    "        ingestion_timestamp = datetime.utcnow()\n",
    "\n",
    "        silver_data = spark.sql(f\"\"\"\n",
    "                                    SELECT\n",
    "                                    r.borough,\n",
    "                                    r.street_name,\n",
    "                                    r.restaurant_id,\n",
    "                                    r.restaurant_name,\n",
    "                                    r.cuisine_type,\n",
    "                                    r.longitude,\n",
    "                                    r.latitude,\n",
    "                                    i.violation_code,\n",
    "                                    v.violation_description,\n",
    "                                    i.inspection_date,\n",
    "                                    i.inspection_type,\n",
    "                                    i.critical_flag,\n",
    "                                    i.score,\n",
    "                                    i.grade,\n",
    "                                    i.action\n",
    "                                    FROM {self.catalog_name}.{self.silver_db_name}.{self.fact_restaurant_inspection} i\n",
    "                                    JOIN {self.catalog_name}.{self.silver_db_name}.{self.dim_restaurant} r \n",
    "                                        ON i.restaurant_id = r.restaurant_id\n",
    "                                    JOIN {self.catalog_name}.{self.silver_db_name}.{self.dim_violation} v\n",
    "                                        ON i.violation_code = v.violation_code\n",
    "                                    where r.borough <> 'NOT AVAILABLE' and r.end_date is null and v.end_date is null\n",
    "                                \"\"\")\n",
    "\n",
    "        if self.table_exists(gold_table_name):\n",
    "            print(f\"ðŸ“Œ Table {gold_table_name} exists. Performing incremental load...\")\n",
    "\n",
    "            key_columns = [col for col in silver_data.columns if col != \"ingestion_timestamp\"]\n",
    "\n",
    "            df = silver_data.withColumn(\"ingestion_timestamp\", lit(ingestion_timestamp))\\\n",
    "                    .withColumn(\"unique_hash\", sha2(concat_ws(\"|\", *[silver_data[col] for col in key_columns]), 256))\\\n",
    "                    .dropDuplicates([\"unique_hash\"])\n",
    "\n",
    "            df.createOrReplaceTempView(\"temp_view\")\n",
    "\n",
    "            existing_df = spark.table(gold_table_name)\n",
    "            existing_df = existing_df.drop(\"ingestion_timestamp\")\n",
    "            print(f\"ðŸ” Existing record count: {existing_df.count()}\")\n",
    "            silver_df = df.drop(\"ingestion_timestamp\")\n",
    "            new_df = silver_df.exceptAll(existing_df)\n",
    "            new_records_count = new_df.count()\n",
    "            print(f\"ðŸ†• New records to insert: {new_records_count}\")\n",
    "\n",
    "            if new_records_count > 0:\n",
    "                spark.sql(f\"\"\"\n",
    "                                MERGE INTO {gold_table_name} AS TARGET\n",
    "                                USING temp_view AS SOURCE\n",
    "                                ON TARGET.unique_hash = SOURCE.unique_hash\n",
    "                                WHEN MATCHED AND TARGET.INGESTION_TIMESTAMP < SOURCE.INGESTION_TIMESTAMP THEN \n",
    "                                UPDATE SET *\n",
    "                                WHEN NOT MATCHED THEN \n",
    "                                INSERT *\n",
    "                            \"\"\")\n",
    "                print(f\"âœ… {new_records_count} new records inserted into {gold_table_name}.\")\n",
    "            else:\n",
    "                print(\"âœ… No new records to insert. Data is already up-to-date.\")\n",
    "\n",
    "        else:\n",
    "            print(f\"ðŸ› ï¸ Table {gold_table_name} does not exist. Creating Table...\")\n",
    "            spark.sql(f\"\"\"\n",
    "                           CREATE TABLE IF NOT EXISTS {gold_table_name} (\n",
    "                                borough STRING,\n",
    "                                street_name STRING,\n",
    "                                restaurant_id INT,\n",
    "                                restaurant_name STRING,\n",
    "                                cuisine_type STRING,\n",
    "                                longitude DOUBLE,\n",
    "                                latitude DOUBLE,\n",
    "                                violation_code STRING,\n",
    "                                violation_description STRING,\n",
    "                                inspection_date DATE,\n",
    "                                inspection_type STRING,\n",
    "                                critical_flag STRING,\n",
    "                                score INT,\n",
    "                                grade STRING,\n",
    "                                action STRING,\n",
    "                                unique_hash STRING,\n",
    "                                ingestion_timestamp TIMESTAMP)\n",
    "                            USING DELTA\n",
    "                            LOCATION '{inspection_detail_storage_path}'\n",
    "                        \"\"\")\n",
    "            print(f\"âœ… Table {gold_table_name} created successfully!\")\n",
    "            print(\"Performing first-time load...\")\n",
    "\n",
    "            key_columns = [col for col in silver_data.columns if col != \"ingestion_timestamp\"]\n",
    "\n",
    "            df = silver_data.withColumn(\"ingestion_timestamp\", lit(ingestion_timestamp))\\\n",
    "                    .withColumn(\"unique_hash\", sha2(concat_ws(\"|\", *[silver_data[col] for col in key_columns]), 256))\\\n",
    "                    .dropDuplicates([\"unique_hash\"])\n",
    "\n",
    "            df.write.mode(\"overwrite\").saveAsTable(gold_table_name)\n",
    "            print(f\"âœ… First-time load completed! {df.count()} rows loaded successfully to {gold_table_name}\")\n",
    "\n",
    "    def inspection_summary(self):\n",
    "        \"\"\"Load NYC Summary inspection data into the Gold layer as a View.\"\"\"\n",
    "    \n",
    "        print(\"ðŸ”¹ Starting gold layer view creation for NYC summarized inspection data...\")\n",
    "\n",
    "        gold_view_name = f\"{self.catalog_name}.{self.db_name}.inspection_summary_view\"\n",
    "        \n",
    "        spark.sql(f\"\"\"\n",
    "            CREATE OR REPLACE VIEW {gold_view_name} AS\n",
    "            SELECT\n",
    "                borough,\n",
    "                COUNT(DISTINCT restaurant_id) AS total_restaurants,\n",
    "                COUNT(restaurant_id) AS total_inspections,\n",
    "                AVG(score) AS avg_inspection_score,\n",
    "                SUM(CASE WHEN critical_flag = 'CRITICAL' THEN 1 ELSE 0 END) AS critical_violations,\n",
    "                SUM(CASE WHEN violation_code <> 'NO VIOLATIONS' THEN 1 ELSE 0 END) AS total_violations,\n",
    "                (SUM(CASE WHEN critical_flag = 'CRITICAL' THEN 1 ELSE 0 END) * 100.0 / NULLIF(COUNT(restaurant_id), 0)) AS pct_critical_violations\n",
    "            FROM {self.catalog_name}.{self.db_name}.inspection_detail\n",
    "            GROUP BY borough\n",
    "        \"\"\")\n",
    "\n",
    "        print(f\"âœ… Successfully created or updated Gold View: {gold_view_name}\")\n",
    "\n",
    "    def inspection_summary_by_borough_population(self):\n",
    "\n",
    "        \"\"\"Load NYC Summary inspection data by Borough Population into the Gold layer as a View.\"\"\"\n",
    "    \n",
    "        print(\"ðŸ”¹ Starting gold layer view creation for NYC summarized inspection data by Borough Population...\")\n",
    "\n",
    "        gold_view_name = f\"{self.catalog_name}.{self.db_name}.inspection_summary_by_borough_population_view\"\n",
    "        \n",
    "        spark.sql(f\"\"\"\n",
    "            CREATE OR REPLACE VIEW {gold_view_name} AS\n",
    "            SELECT \n",
    "                p.borough,\n",
    "                p.year,\n",
    "                ir.total_restaurants,\n",
    "                ir.total_inspections,\n",
    "                ir.avg_inspection_score,\n",
    "                ir.critical_violations,\n",
    "                ir.total_violations,\n",
    "                ir.pct_critical_violations,\n",
    "                sum(p.population) as total_borough_population\n",
    "            FROM {self.catalog_name}.{self.silver_db_name}.dim_borough_population p\n",
    "            LEFT JOIN {self.catalog_name}.{self.db_name}.inspection_summary_view ir\n",
    "                ON p.borough = ir.borough\n",
    "            GROUP BY 1,2,3,4,5,6,7,8\n",
    "            ORDER BY 1,2\n",
    "        \"\"\")\n",
    "\n",
    "        print(f\"âœ… Successfully created or updated Gold View: {gold_view_name}\")\n",
    "\n",
    "    def violation_summary_by_cuisine(self):\n",
    "\n",
    "        \"\"\"Load NYC Violation data by Cuisine types into the Gold layer as a View.\"\"\"\n",
    "    \n",
    "        print(\"ðŸ”¹ Starting gold layer view creation for NYC Violation data by Cuisine types...\")\n",
    "\n",
    "        gold_view_name = f\"{self.catalog_name}.{self.db_name}.violation_summary_by_cuisine_view\"\n",
    "        \n",
    "        spark.sql(f\"\"\"\n",
    "            CREATE OR REPLACE VIEW {gold_view_name} AS\n",
    "            SELECT \n",
    "                cuisine_type,\n",
    "                violation_code,\n",
    "                violation_description,\n",
    "                COUNT(restaurant_id) AS total_violations,\n",
    "                SUM(CASE WHEN critical_flag = 'CRITICAL' THEN 1 ELSE 0 END) AS critical_violations   \n",
    "            FROM {self.catalog_name}.{self.db_name}.inspection_detail\n",
    "            GROUP BY 1,2,3\n",
    "        \"\"\")\n",
    "\n",
    "        print(f\"âœ… Successfully created or updated Gold View: {gold_view_name}\")\n",
    "\n",
    "    def best_restaurant_locations(self):\n",
    "\n",
    "        \"\"\"Load NYC best restaurant data into the Gold layer as a View.\"\"\"\n",
    "    \n",
    "        print(\"ðŸ”¹ Starting gold layer view creation for NYC best restaurant locations...\")\n",
    "\n",
    "        gold_view_name = f\"{self.catalog_name}.{self.db_name}.best_restaurant_locations_view\"\n",
    "        \n",
    "        spark.sql(f\"\"\"\n",
    "            CREATE OR REPLACE VIEW {gold_view_name} AS\n",
    "                with restaurant_distribution as (\n",
    "                    select \n",
    "                        borough,\n",
    "                        count(distinct restaurant_id) as total_restaurants,\n",
    "                        avg(score) as avg_inspection_score\n",
    "                    from {self.catalog_name}.{self.db_name}.inspection_detail\n",
    "                    group by borough\n",
    "                    )\n",
    "                , population_data as (\n",
    "                    select \n",
    "                        borough,\n",
    "                        sum(population) as population\n",
    "                    from {self.catalog_name}.{self.silver_db_name}.dim_borough_population\n",
    "                    where year = (select max(year) from {self.catalog_name}.{self.silver_db_name}.dim_borough_population)\n",
    "                    group by borough\n",
    "                    )\n",
    "\n",
    "                SELECT \n",
    "                    p.borough,\n",
    "                    p.population,\n",
    "                    rd.total_restaurants,\n",
    "                    (rd.total_restaurants * 1.0 / NULLIF(p.population, 0)) * 1000 AS restaurants_per_1000_people,\n",
    "                    rd.avg_inspection_score\n",
    "                FROM population_data p\n",
    "                LEFT JOIN restaurant_distribution rd \n",
    "                    ON p.borough = rd.borough\n",
    "                ORDER BY restaurants_per_1000_people ASC\n",
    "            \"\"\")\n",
    "\n",
    "        print(f\"âœ… Successfully created or updated Gold View: {gold_view_name}\")\n",
    "\n",
    "    \n",
    "    def gold_layer_execution(self):\n",
    "\n",
    "        setup = Setup(\"gold_db\")\n",
    "        setup.create_db()\n",
    "\n",
    "        self.inspection_detail()\n",
    "        self.inspection_summary()\n",
    "        self.inspection_summary_by_borough_population()\n",
    "        self.violation_summary_by_cuisine()\n",
    "        self.best_restaurant_locations()\n",
    "\n",
    "        print(f\"ðŸŽ¯ Data processing for Gold completed successfully.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4e6127f7-de43-4ead-b60d-e3b90b50f6e2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "gold = Gold()\n",
    "gold.gold_layer_execution()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 3274393618467330,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "05-gold",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
